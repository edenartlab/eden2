name: Create an image (SDXL)
description: Generate an image from text using our classic SDXL model and trained concepts.
tip: |-
  This is the most basic and common function which generates an image from text a prompt, with ControlNet guidance to maintain the contours or shape outlines of the input image while generating a new style or aesthetic on top of it, LoRA models to apply a pretrained fine tuned concept, and ipAdapter functionality to transfer the style from a Style Image to the output result.
thumbnail: app/txt2img.png
cost_estimate: 1 * n_samples
status: prod
visible: null
output_type: image
resolutions: [21-9_1536x640, 16-9_1344x768, 3-2_1216x832, 4-3_1152x896, 5-4_1088x896, 1-1_1024x1024, 4-5_896x1088, 3-4_896x1152, 2-3_832x1216, 9-16_768x1344, 9-21_640x1536]
comfyui_output_node: 146
comfyui_intermediate_outputs:
  controlnet_signal: 323
parameters:
  prompt:
    type: str
    label: Prompt
    description: Describe an image
    tip: "Prompts are specific, detailed, concise, and visually descriptive, avoiding unnecessary verbosity and abstract, generic terms. They usually have \u25CF Primary subject (i.e., person, character, animal, object, ...), e.g \"Renaissance noblewoman\", \"alien starship\". Should appear early in prompt. \u25CF Action of the subject, e.g. \"holding an ancient book\", \"orbiting a distant planet\", if there is no main subject, a good context description will do here (what is going on in the scene?). They sometimes have several stylistic modifiers near the end of the prompt, such as \u25CF Background, environment or context surrounding the subject, e.g. \"in a dimly lit Gothic castle\", \"in a futuristic 22nd century space station\". \u25CF Secondary items that enhance the subject or story. e.g. \"wearing an intricate lace collar\", \"standing next to a large, ancient tree\". \u25CF Color schemes, e.g. \"shades of deep red and gold\", \"monochrome palette with stark contrasts\", \"monochrome\", ... \u25CF Style or method of rendering, e.g. \"reminiscent of Vermeer's lighting techniques\", \"film noir\", \"cubism\", ... \u25CF Mood or atmospheric quality e.g. \"atmosphere of mystery\", \"serene mood\". \u25CF Lighting conditions e.g. \"bathed in soft, natural window light\", \"dramatic shadows under a spotlight\". \u25CF Perspective or or viewpoint, e.g. \"bird's eye view\", \"from a low angle\", \"fisheye\", .. \u25CF Textures or materials, e.g. \"textures of rich velvet and rough stone\". \u25CF Time Period, e.g. \"Victorian Era\", \"futuristic 22nd century\". \u25CF Cultural elements, e.g. \"inspired by Norse mythology\", \"traditional Japanese setting\". \u25CF Artistic medium, e.g. \"watercolor painting\", \"crisp digital Unreal Engine rendering\", \"8K UHD professional photo\", \"cartoon drawing\", ... They often end with trigger words that improve images in a general way, e.g. \"High Resolution\", \"HD\", \"sharp details\", \"masterpiece\", \"stunning composition\", ... If the prompt contains a request to render text, enclose the text in quotes, e.g. A Sign with the text \u201CPeace\u201D. IMPORTANT, If the user gives you a short, general, or visually vague prompt, you should augment their prompt by imagining richer details, following the tips above. If a user gives a long, detailed, or well-thought out composition, or requests to have their prompt strictly adhered to without revisions or embellishment, you should adhere to or repeat their exact prompt. The goal is to be authentic to the user's request, but to help them get better results when they are new, unsure, or lazy."
    required: true
    comfyui:
      node_id: 370
      field: inputs
      subfield: body
  n_samples:
    type: int
    label: Number of samples
    description: Number of samples to generate
    tip: |-
      This is the number of tries to generate for the prompt. If you get a request for n_samples > 1, you are still using a *single* prompt for the whole set.
    required: true
    default: 1
    minimum: 1
    maximum: 4
  use_init_image:
    type: bool
    label: Use starting image
    description: Enable image-to-image, and controlnet guidance features
    tip: |-
      Using a starting image helps guide your prompted creation towards the elements present in your initial input. if an image is supplied with the prompt or controlnet is employed, this should be true.
    default: false
    comfyui:
      node_id: 198
      field: inputs
      subfield: value
  init_image:
    type: image
    label: Starting image
    description: Initial image from which to start diffusion process
    visible_if: use_init_image=true
    comfyui:
      node_id: 16
      field: inputs
      subfield: image
  size_from_input:
    type: bool
    label: Use starting image size
    description: |-
      Override the width and height parameters with the actual resolution of your starting image.
    tip: |-
      It's best practice to use resolutions included in the models training data. Upscaling later can get you to a higher resolution.
    default: true
    visible_if: use_init_image=true
    comfyui:
      node_id: 8
      field: inputs
      subfield: value
  enforce_SDXL_resolution:
    type: bool
    label: Use optimized resolution
    description: Round to the nearest optimized SDXL resolution.
    tip: |-
      It's best practice to use resolutions included in the models training data. This option will fill or crop the input image to a legal SDXL resolution.
    default: false
    visible_if: use_init_image=true
    comfyui:
      node_id: 337
      field: inputs
      subfield: value
  denoise:
    type: float
    label: Generation strength
    description: Strength of the generation process on top of the starting image
    tip: |-
      Decreasing generation strength increases the influence of a starting image in image-to-image workflows. The default of 1 is 100% AI creativity, ignoring all traces of the starting image, whereas a medium blend of about 50% will maintain close adherance to the original input. Denoise should always stay at 1 unless an init image is employed; never use less than 1 unless you're specifically trying to do an img2img task.
    default: 1.0
    minimum: 0.0
    maximum: 1.0
    visible_if: use_init_image=true
    comfyui:
      node_id: 220
      field: inputs
      subfield: value
  use_lora:
    type: bool
    label: Use LoRA
    description: Apply LoRA finetune model style to image generation
    tip: |-
      Models created with Eden LoRA trainer can add people, styles and conceptual embeddings into the diffusion model, giving it an idea of new information provided by the user.
    default: false
    comfyui:
      node_id: 193
      field: inputs
      subfield: value
  lora:
    type: lora
    label: LoRA
    description: Use a LoRA finetune on top of the base model.
    visible_if: use_lora=true
    comfyui:
      node_id: 162
      field: inputs
      subfield: lora_name
  lora_strength:
    type: float
    label: LoRA strength
    description: Strength of the LoRA
    tip: |-
      If outputs resemble the LoRA but have low prompt adherence or all look the same, turn down the LoRA strength.
    default: 0.5
    minimum: 0.0
    maximum: 1.5
    visible_if: use_lora=true
    comfyui:
      node_id: 96
      field: inputs
      subfield: value
  token_scale:
    type: float
    label: embedding strength
    description: Strength of the learned LoRA token that represents your concept
    tip: |-
      This parameter controls the influence of the learned textual inversion embedding on the prompt conditioning. This should typically be around 0.7-1.0
    default: 1.0
    minimum: 0.0
    maximum: 1.25
    visible_if: use_lora=true
    hide_from_ui: true
    comfyui:
      node_id: 312
      field: inputs
      subfield: value
  use_controlnet:
    type: bool
    label: Use controlnet
    visible_if: use_init_image=true
    description: Apply Controlnet guidance to the image generation.
    tip: |-
      Controlnet uses an assortment of image preprocessors to guide the output results towards the shape of a Starting Image.
    default: false
    comfyui:
      node_id: 307
      field: inputs
      subfield: a
  preprocessor:
    type: str
    label: Controlnet preprocessor
    description: |-
      the preprocessor you choose will prepare your input image to guide your diffusion towards that shape
    tip: |-
      depth will recreate an approximate depth of your input scene, and is best for 3 dimensional compositions. Canny edge creates strong edge detection adherance to the shape of your image, with cannyv2 as an updated model that's sharper but with less detail. The scribble model will create guidance towards a rougher sketched shape of your starting image. pose follows an open pose skeleton of a character's body position. threshold creates a hard black and white matte from the input image, and lineart provides a crisp hard edge guidance from the input.
    default: canny
    choices: [none, cannyv2, canny, depth, scribble, pose, threshold, lineart]
    visible_if: use_controlnet=true
    comfyui:
      node_id: 283
      field: inputs
      subfield: select_item
  controlnet_strength:
    type: float
    label: Controlnet strength
    description: set the guidance strength of the controlnet model in slot 1
    tip: |-
      A good default is around 0.6, with ranges between 0.2 for subtle guidance, and 1.0 for something more heavy handed
    default: 0.7
    minimum: 0.0
    maximum: 1.5
    visible_if: use_controlnet=true
    comfyui:
      node_id: 36
      field: inputs
      subfield: value
  use_ipadapter:
    type: bool
    label: Use style image
    description: Transfer style from image
    tip: |-
      Ipadapter takes compositional concepts, subjects and aesthetic elements from a style image and applies it to the generated image.
    default: false
    comfyui:
      node_id: 35
      field: inputs
      subfield: value
  style_image:
    type: image
    label: Style image
    description: |-
      Image of an aesthetic style, texture or visual content transferred to the generated image creation
    visible_if: use_ipadapter=true
    comfyui:
      node_id: 145
      field: inputs
      subfield: image
  ipadapter_strength:
    type: float
    label: Style strength
    description: set the strength of the image style transfer
    tip: |-
      Higher values (>0.8) will cause strong adherence to the style image, lower values (<0.3) will give more freedom to interpret the prompt.
    default: 0.6
    minimum: 0.0
    maximum: 1.5
    visible_if: use_ipadapter=true
    comfyui:
      node_id: 59
      field: inputs
      subfield: value
  ipadapter_mode:
    type: str
    label: Ipadapter style transfer mode
    description: |-
      Allows for different types of style transfer using ipadapter, each with different characteristics and more or less content blending.
    tip: |-
      All of these are variations of the same idea, but use different weighting schemas for injecting the style embedding into the unet. Each has different tradeoffs for style transfer versus content blending. Ease in-out is a great default, strong style transfer has less content bleeding.
    visible_if: use_ipadapter=true
    default: strong style transfer
    choices: [ease in-out, style transfer, strong style transfer, style transfer precise, composition precise]
    comfyui:
      node_id: 141
      field: inputs
      subfield: weight_type
  width:
    type: int
    label: Width
    description: Width in pixels
    tip: |-
      Default to using a high resolution of at least 1 megapixel for the image. Use landscape aspect ratio for prompts that are wide or more landscape-oriented, and portrait aspect ratio for tall things. When using portrait aspect ratio, do not exceed 1:1.5 aspect ratio. Only do square if the user requests it. Use your best judgment.
    default: 1024
    minimum: 256
    maximum: 2048
    visible_if: use_init_image=false
    step: 8
    comfyui:
      node_id: 7
      field: inputs
      subfield: width
  height:
    type: int
    label: Height
    description: Height in pixels
    tip: Use the description for width to understand how to use height well.
    default: 1024
    minimum: 256
    maximum: 2048
    visible_if: use_init_image=false
    step: 8
    comfyui:
      node_id: 7
      field: inputs
      subfield: height
  negative_prompt:
    type: str
    label: Negative prompt
    description: Negative text prompt, what you do *not* want to generate.
    tip: |-
      Keep this at default or at most, add to this prompt, unless specifically instructed otherwise.
    default: embedding:negativeXL_D, (worst quality, low quality, normal quality:1.5)
    comfyui:
      node_id: 33
      field: inputs
      subfield: value
  seed:
    type: int
    label: Seed
    description: Set random seed for reproducibility. If blank, will be set to a random value.
    tip: |-
      You should only set this if you want to start from/copy the seed of a previous image. Unless one is specified, you should leave this blank. Great for redoing an image generation you like with a slightly modified prompt, but otherwise you should usually leave this blank.
    default: random
    minimum: 0
    maximum: 2147483647
    comfyui:
      node_id: 27
      field: inputs
      subfield: seed
  steps:
    type: int
    label: Steps
    description: Set the number of diffusion steps
    tip: |-
      The default value is sufficient for most jobs, but at a speed cost can be increased, or decreased to undercook the image generation.
    default: 30
    step: 1
    minimum: 10
    maximum: 50
    comfyui:
      node_id: 171
      field: inputs
      subfield: steps_total
  cfg:
    type: float
    label: Guidance
    description: CFG strength affects the influence of the prompt on the image generation.
    tip: |-
      Classifier Free Guidance strength should usually be set to the default, but can be increased or decreased within the range presented to increase or decrease the affect of the prompt on the creation.
    default: 8.0
    step: 0.1
    minimum: 1.0
    maximum: 12.0
    comfyui:
      node_id: 171
      field: inputs
      subfield: cfg
